{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Project: Source Separation of Piano Concertos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import mir_eval\n",
    "from spleeter.__main__ import separate\n",
    "from glob import glob\n",
    "from typing import Dict, Tuple\n",
    "from spleeter.audio import Codec\n",
    "\n",
    "SAMPLE_RATE = 22050\n",
    "MUS_DIR = \"metadata\" # where you put the music metadata\n",
    "OUTPUT_DIR = \"data\" # where you want to put the output dataset\n",
    "CONFIG_DIR = \"configs\" # where you put the configuration files\n",
    "N_TRAIN = 1200\n",
    "N_VAL = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Processing the dataset for pretraining.\n",
    "\n",
    "The directory containing the metadata must be in the form \".../{Composer}/{Piano/Orchestra}/{audiofiles}\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHgQV9_gEXbo",
    "outputId": "09024aae-ce36-4610-89ca-a28a5c153d51"
   },
   "outputs": [],
   "source": [
    "def trim_audio(wave_path, duration_sec = 20):\n",
    "    \"\"\"\n",
    "    Trim the audio wave into 20-sec chunks.\n",
    "    \"\"\"\n",
    "    clips = []\n",
    "    waveform, _ = librosa.load(wave_path, sr = SAMPLE_RATE)\n",
    "    total_samples = waveform.shape[0]\n",
    "    num_clips = total_samples // (duration_sec * SAMPLE_RATE)\n",
    "\n",
    "    for i in range(num_clips):\n",
    "        start_sample = i * duration_sec * SAMPLE_RATE\n",
    "        end_sample = start_sample + duration_sec * SAMPLE_RATE\n",
    "        clip_waveform = waveform[start_sample:end_sample]\n",
    "        clips.append(clip_waveform)\n",
    "    return clips\n",
    "\n",
    "def mix_audio(piano_waves, orch_waves, composer, train, validation, dur = 20.0):\n",
    "    \"\"\"\n",
    "    Randomly mixing the piano and orchestra audio waves.\n",
    "    piano_waves: waveform of piano, type: numpy array\n",
    "    orch_waves: waveform of orchestra, must be the same shape as piano_waves\n",
    "    composer: composer, type: string\n",
    "    train: a pandas dataframe recording the training set\n",
    "    validation: a pandas dataframe recording the validation set\n",
    "    dur: duration of piano_waves and orch_waves in seconds\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(N_TRAIN + N_VAL)):\n",
    "      piano_idx = np.random.randint(low = 0, high = len(piano_waves))\n",
    "      orch_idx = np.random.randint(low = 0, high = len(orch_waves))\n",
    "      piano_sample = piano_waves[piano_idx]\n",
    "      orch_sample = orch_waves[orch_idx]\n",
    "      mix_sample = (piano_sample + orch_sample) / 2\n",
    "      if i < N_TRAIN:\n",
    "        save_path = os.path.join(OUTPUT_DIR, \"train\", composer + \"_\" + str(i))\n",
    "        train.loc[len(train)] = {\n",
    "          \"mix_path\": os.path.join(save_path, \"mix.wav\"),\n",
    "          \"piano_path\": os.path.join(save_path, \"piano.wav\"),\n",
    "          \"orchestra_path\": os.path.join(save_path, \"orchestra.wav\"),\n",
    "          \"duration\": dur\n",
    "      }\n",
    "      else :\n",
    "        save_path = os.path.join(OUTPUT_DIR, \"val\", composer + \"_\" + str(i))\n",
    "        validation.loc[len(validation)] = {\n",
    "          \"mix_path\": os.path.join(save_path, \"mix.wav\"),\n",
    "          \"piano_path\": os.path.join(save_path, \"piano.wav\"),\n",
    "          \"orchestra_path\": os.path.join(save_path, \"orchestra.wav\"),\n",
    "          \"duration\": dur\n",
    "        }\n",
    "      os.makedirs(save_path, exist_ok = True)\n",
    "      sf.write(os.path.join(save_path, \"piano.wav\"), piano_sample, SAMPLE_RATE)\n",
    "      sf.write(os.path.join(save_path, \"orchestra.wav\"), orch_sample, SAMPLE_RATE)\n",
    "      sf.write(os.path.join(save_path, \"mix.wav\"), mix_sample, SAMPLE_RATE)\n",
    "\n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmRArkcnU4sR"
   },
   "outputs": [],
   "source": [
    "def make_dataset(train_name, val_name):\n",
    "    \"\"\"\n",
    "    Mix samples and write Spleeter-required csv files for training and testing set.\n",
    "    \"\"\"\n",
    "    train = pd.DataFrame(columns = [\"mix_path\", \"piano_path\", \"orchestra_path\", \"duration\"])\n",
    "    validation = pd.DataFrame(columns = [\"mix_path\", \"piano_path\", \"orchestra_path\", \"duration\"])\n",
    "\n",
    "    for composer in os.listdir(MUS_DIR):\n",
    "      if composer[0] == \".\":\n",
    "        continue\n",
    "      piano_path = os.path.join(MUS_DIR, composer, \"Piano\")\n",
    "      orch_path = os.path.join(MUS_DIR, composer, \"Orchestra\")\n",
    "      piano_waves = []\n",
    "      orch_waves = []\n",
    "      for piece in os.listdir(piano_path):\n",
    "        if not piece[0] == \".\":\n",
    "          waves = trim_audio(os.path.join(piano_path, piece))\n",
    "          piano_waves.extend(waves)\n",
    "      for piece in os.listdir(orch_path):\n",
    "        if not piece[0] == \".\":\n",
    "          waves = trim_audio(os.path.join(orch_path, piece))\n",
    "          orch_waves.extend(waves)\n",
    "      train, validation = mix_audio(piano_waves, orch_waves, composer = composer, train = train, validation = validation)\n",
    "\n",
    "\n",
    "    os.makedirs(CONFIG_DIR, exist_ok = True)\n",
    "    train.to_csv(os.path.join(CONFIG_DIR, train_name), index = False)\n",
    "    validation.to_csv(os.path.join(CONFIG_DIR, val_name), index = False)\n",
    "\n",
    "\n",
    "make_dataset(\"train.csv\", \"val.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating TTA data for each test recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_and_concatenate(wave_time):\n",
    "    \"\"\"\n",
    "    Trim the piano-only and orchestra-only passages from the test recording.\n",
    "    wave_time: {filename: [[start_1, end_1], [start_2, end_2], ...]}\n",
    "    \"\"\"\n",
    "    clips = []\n",
    "    concatenated = []\n",
    "    for wave_path, time_list in wave_time.items():\n",
    "      waveform, _ = librosa.load(wave_path, sr = SAMPLE_RATE)\n",
    "\n",
    "      for time in time_list:\n",
    "          begin, end = time\n",
    "          begin_sample = begin * SAMPLE_RATE\n",
    "          end_sample = end * SAMPLE_RATE\n",
    "          clip_waveform = waveform[begin_sample:end_sample]\n",
    "          clips.append(clip_waveform)\n",
    "        \n",
    "      concatenated.append(np.concatenate(clips, axis=0))\n",
    "    \n",
    "    return np.concatenate(concatenated, axis=0)\n",
    "\n",
    "piano_wave_time = {\n",
    "   \"BeethovenOp73.mp3\":[[0, 100], [100, 200]]\n",
    "}\n",
    "\n",
    "orchestra_wave_time = {\n",
    "   \"BeethovenOp73.mp3\":[[200, 300]]\n",
    "}\n",
    "\n",
    "\n",
    "piano_only = trim_audio(trim_and_concatenate(piano_wave_time))\n",
    "orchestra_only = trim_audio(trim_and_concatenate(orchestra_wave_time))\n",
    "\n",
    "train = pd.DataFrame(columns = [\"mix_path\", \"piano_path\", \"orchestra_path\", \"duration\"])\n",
    "validation = pd.DataFrame(columns = [\"mix_path\", \"piano_path\", \"orchestra_path\", \"duration\"])\n",
    "\n",
    "train, validation = mix_audio(piano_only, orchestra_only, n_train = 100, n_val = 5, piece_name = \"BeethovenOp73\", train = train, validation = validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_SPLIT: str = \"val\"\n",
    "EVALUATION_METRICS_DIRECTORY: str = \"metrics\"\n",
    "EVALUATION_INSTRUMENTS: Tuple[str, ...] = (\"piano\", \"orchestra\")\n",
    "EVALUATION_MIXTURE: str = \"mix.wav\"\n",
    "EVALUATION_AUDIO_DIRECTORY: str = \"audio\"\n",
    "\n",
    "\n",
    "def calculate_sdr(reference_audio_path, estimation_audio_path):\n",
    "    \"\"\"\n",
    "    SDR calculation using mir_eval package.\n",
    "    \"\"\"\n",
    "    reference, _ = librosa.load(reference_audio_path, sr=None)\n",
    "    estimation, _ = librosa.load(estimation_audio_path, sr=None)\n",
    "    sdr, _, _, _ = mir_eval.separation.bss_eval_sources(reference[None, :], estimation[None, :], compute_permutation=False)\n",
    "    return sdr[0]\n",
    "\n",
    "def evaluate(\n",
    "    output_path: str,\n",
    "    params_filename: str,\n",
    "    mus_dir: str,\n",
    "    verbose: bool = 0,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a model on the test recording and print out the SDR values for piano and orchestra.\n",
    "    \"\"\"\n",
    "    songs = glob(os.path.join(mus_dir, EVALUATION_SPLIT, \"*/\"))\n",
    "    mixtures = [os.path.join(song, EVALUATION_MIXTURE) for song in songs]\n",
    "    audio_output_directory = os.path.join(output_path, EVALUATION_SPLIT)\n",
    "    separate(\n",
    "        deprecated_files=None,\n",
    "        files=mixtures,\n",
    "        adapter=\"spleeter.audio.ffmpeg.FFMPEGProcessAudioAdapter\",\n",
    "        bitrate=\"128k\",\n",
    "        codec=Codec.WAV,\n",
    "        duration=600.0,\n",
    "        offset=0,\n",
    "        output_path=audio_output_directory,\n",
    "        filename_format=\"{foldername}/{instrument}.{codec}\",\n",
    "        params_filename=params_filename,\n",
    "        mwf=False,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    \n",
    "\n",
    "    for instrument in EVALUATION_INSTRUMENTS:\n",
    "        sdr_values = []\n",
    "        for piece in os.listdir(os.path.join(mus_dir, EVALUATION_SPLIT)):\n",
    "            if piece[0] == \".\":\n",
    "                continue\n",
    "            est_path = os.path.join(audio_output_directory, piece, f\"{instrument}.wav\")\n",
    "            ref_path = os.path.join(mus_dir, EVALUATION_SPLIT, piece, f\"{instrument}.wav\")\n",
    "            sdr_values.append(calculate_sdr(ref_path, est_path))\n",
    "        print(f\"{instrument}_sdr: {np.median(sdr_values)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Pretraining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spleeter train -p configs/base_config.json -d data --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Test-time adaptation\n",
    "\n",
    "Each time, modify \"train_csv\", \"validation_csv\" and \"model_dir\" in base_config_{piece}.json and base_config_{piece}_test.json and put them under the configs directory. Then copy and rename the folder of pretrained model parameters and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r pretrained B73\n",
    "!spleeter train -p configs/base_config_B73.json -d /home/featurize/data --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(output_path = \"audio\", params_filename = \"configs/base_config.json\", mus_dir = \"data/BeethovenOp73\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(output_path = \"audio\", params_filename = \"configs/base_config_B73_test.json\", mus_dir = \"data/BeethovenOp73\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
